{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offense_rating_simple_model_dense_layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmhHcKcjPapl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b361707b-b39c-4e8d-dbc2-701f78782fea"
      },
      "source": [
        "# connect with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFiBr2XqSGD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a2959f-d735-4485-d741-60f7a477b397"
      },
      "source": [
        "!pip install keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "import string\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK3CQCMCQKF_"
      },
      "source": [
        "def load_dataset():\n",
        "  column_names = [ 'id', 'text', 'is_humor', 'humor_rating', 'humor_controversy', 'offense_rating' ]\n",
        "  data_path = '/content/drive/My Drive/data-nlp-humor-offense/train.csv'\n",
        "  data_frame = pd.read_csv(data_path, names=column_names, skiprows=1, na_values=\"?\", sep=\",\", skipinitialspace=True)\n",
        "  data = data_frame.fillna(0).to_numpy()\n",
        "\n",
        "  return data, data_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdL8dNRjQto-",
        "outputId": "4b412b4f-dbc2-442d-8cd3-e69a2989f2df"
      },
      "source": [
        "result, df_result = load_dataset()\n",
        "print(result[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1\n",
            "  \"TENNESSEE: We're the best state. Nobody even comes close. *Elevennessee walks into the room* TENNESSEE: Oh shit...\"\n",
            "  1 2.42 1.0 0.2]\n",
            " [2\n",
            "  'A man inserted an advertisement in the classifieds \"Wife Wanted\". The next day, he received 1000 of replies, all reading: \"You can have mine.\" Free delivery also available at your door step'\n",
            "  1 2.5 1.0 1.1]\n",
            " [3\n",
            "  'How many men does it take to open a can of beer? None. It should be open by the time she brings it to the couch.'\n",
            "  1 1.95 0.0 2.4]\n",
            " [4\n",
            "  \"Told my mom I hit 1200 Twitter followers. She pointed out how my brother owns a house and I'm wanted by several collection agencies. Oh ma!\"\n",
            "  1 2.11 1.0 0.0]\n",
            " [5\n",
            "  'Roses are dead. Love is fake. Weddings are basically funerals with cake.'\n",
            "  1 2.78 0.0 0.1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXTpvXFcfo1x",
        "outputId": "2f6c71b3-b43e-4b89-f7ae-91c476b3d8bb"
      },
      "source": [
        "print(df_result['is_humor'].value_counts())\n",
        "print(df_result['humor_rating'].value_counts())\n",
        "print(df_result['humor_controversy'].value_counts())\n",
        "print(df_result['offense_rating'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    4932\n",
            "0    3068\n",
            "Name: is_humor, dtype: int64\n",
            "2.00    156\n",
            "2.50    125\n",
            "2.05    111\n",
            "2.60    109\n",
            "2.25    101\n",
            "       ... \n",
            "3.13      1\n",
            "0.47      1\n",
            "2.57      1\n",
            "3.74      1\n",
            "0.30      1\n",
            "Name: humor_rating, Length: 253, dtype: int64\n",
            "0.0    2467\n",
            "1.0    2465\n",
            "Name: humor_controversy, dtype: int64\n",
            "0.00    3388\n",
            "0.15     394\n",
            "0.05     387\n",
            "0.10     333\n",
            "0.20     306\n",
            "        ... \n",
            "4.85       2\n",
            "4.80       2\n",
            "4.55       2\n",
            "4.75       1\n",
            "4.45       1\n",
            "Name: offense_rating, Length: 98, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Go-yTSizFd"
      },
      "source": [
        "def format_text_to_one_hot(array_of_texts):\n",
        "  vocabulary = []\n",
        "  vocabulary_map = {}\n",
        "  counter = 1\n",
        "  end_result = []\n",
        "  texts_scores = []\n",
        "\n",
        "  for text in array_of_texts:\n",
        "    \n",
        "    text_represented_with_numbers = []\n",
        "    positive_text_score = 0\n",
        "    negative_text_score = 0\n",
        "    # break the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # break the sentence into words\n",
        "      words_array = word_tokenize(sentence)\n",
        "\n",
        "      for word in words_array:\n",
        "\n",
        "        # remove punctuation\n",
        "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "        \n",
        "        # lower case all letters \n",
        "        word = word.lower()\n",
        "\n",
        "        if word != '' and word != 's':\n",
        "          if not vocabulary.__contains__(word):\n",
        "            vocabulary.append(word)\n",
        "            vocabulary_map[word] = counter\n",
        "            counter += 1\n",
        "          \n",
        "          text_represented_with_numbers.append(vocabulary_map[word])\n",
        "\n",
        "        # add positive negative score\n",
        "\n",
        "    # add text to result array\n",
        "    end_result.append(text_represented_with_numbers)\n",
        "   # difference = positive_text_score/negative_text_score\n",
        "   # texts_scores.append(difference)\n",
        "  \n",
        "  # arrays to have same length\n",
        "  padded_seq = pad_sequences(end_result, maxlen=63, dtype='int32', padding='pre', value=0.0)\n",
        "  print(len(vocabulary))\n",
        "  return padded_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gy0eR2WK4gX"
      },
      "source": [
        "# n - NOUN\n",
        "#v - VERB\n",
        "#a - ADJECTIVE\n",
        "#s - ADJECTIVE SATELLITE\n",
        "#r - ADVERB\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
        "def get_sentiment(word,tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    \n",
        "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        return []\n",
        "\n",
        "    #Lemmatization\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    if not lemma:\n",
        "        return []\n",
        "\n",
        "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
        "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
        "    #Some of the words have only one Synset and some have several.\n",
        "    synsets = wn.synsets(word, pos=wn_tag)\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
        "\n",
        "    pos=neg=obj=count=0\n",
        "    \n",
        "\n",
        "def getPostTags(array_of_texts):\n",
        "  vocabulary = []\n",
        "  vocabulary_map = {}\n",
        "  counter = 1\n",
        "  end_result = []\n",
        "  texts_scores = []\n",
        "  postagging = []\n",
        "\n",
        "  for text in array_of_texts:\n",
        "    \n",
        "    text_represented_with_numbers = []\n",
        "    positive_text_score = 0\n",
        "    negative_text_score = 0\n",
        "    # break the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    posttagging_sentences = []\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # break the sentence into words\n",
        "      words_array = word_tokenize(sentence)\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      filtered_sentence = [w for w in words_array if not w in stop_words] \n",
        "\n",
        "      # remove punctuation\n",
        "      word_tokens2 = [w for w in filtered_sentence if not w in string.punctuation]\n",
        "      # lower case all letters \n",
        "      lower_case_sentance = map(lambda w: w.lower(), word_tokens2)\n",
        "      #lemmatization\n",
        "      lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
        "        \n",
        "      # Remove characters which have length less than 2  \n",
        "      without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
        "      # Remove numbers\n",
        "      cleaned_data = [word for word in without_single_chr if not word.isnumeric()]\n",
        "      \n",
        "      posttagging_sentences.append(nltk.pos_tag(cleaned_data))\n",
        "    \n",
        "    postagging.append(posttagging_sentences)\n",
        "  return postagging\n",
        "\n",
        "    ###################################################################################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2aHzQdWn9aM"
      },
      "source": [
        "# value = [\"Something, this is an example. One!\", \"This is two!! Really, that's exactly what i though.\"]\n",
        "# format_text_to_one_hot(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U-OqRxCqIe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f08401-df44-49fa-887a-48ce49cbbb98"
      },
      "source": [
        "texts = df_result['text']\n",
        "values = df_result['humor_rating']\n",
        "np.save('/tmp/values', values)\n",
        "print(len(texts))\n",
        "with tf.device('/GPU:0'):\n",
        "  formatted_texts = format_text_to_one_hot(texts)\n",
        "np.save('/tmp/formatted_texts', formatted_texts)\n",
        "print(len(formatted_texts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "15050\n",
            "8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObxWn1kpgigB",
        "outputId": "c89d8700-9227-4a04-d299-36c5d0a94e22"
      },
      "source": [
        "senti_score = []\n",
        "pos=neg=obj=count=0\n",
        "with tf.device('/GPU:0'):\n",
        "  posttagging = getPostTags(texts)\n",
        "print(posttagging[:5])\n",
        "count1 = 1\n",
        "for text in posttagging:\n",
        "  senti_scores_text = 0\n",
        "  for sentence in text:\n",
        "    \n",
        "    senti_val = [get_sentiment(x,y) for (x,y) in sentence]\n",
        "    for score in senti_val:\n",
        "      try:\n",
        "        pos = pos + score[1]  #positive score is stored at 2nd position\n",
        "        neg = neg + score[2]  #negative score is stored at 3rd position\n",
        "      except:\n",
        "        continue\n",
        "    result_score = pos - neg\n",
        "    senti_scores_text += result_score \n",
        "    pos=neg=0 \n",
        "  senti_score.append(senti_scores_text)\n",
        "\n",
        "print(senti_score[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[('TENNESSEE', 'NNP'), (\"'re\", 'VBP'), ('best', 'JJS'), ('state', 'NN')], [('Nobody', 'NN'), ('even', 'RB'), ('come', 'VBP'), ('close', 'RB')], [('*Elevennessee', 'NN'), ('walk', 'NN'), ('room*', 'NN'), ('TENNESSEE', 'NNP'), ('shit', 'NN'), ('...', ':')]], [[('man', 'NN'), ('inserted', 'VBD'), ('advertisement', 'JJ'), ('classified', 'JJ'), ('Wife', 'NNP'), ('Wanted', 'VBD')], [('The', 'DT'), ('next', 'JJ'), ('day', 'NN'), ('received', 'VBD'), ('reply', 'RB'), ('reading', 'VBG'), ('You', 'PRP'), ('mine', 'VBP')], [('Free', 'JJ'), ('delivery', 'NN'), ('also', 'RB'), ('available', 'JJ'), ('door', 'JJ'), ('step', 'NN')]], [[('How', 'WRB'), ('many', 'JJ'), ('men', 'NNS'), ('take', 'VBP'), ('open', 'JJ'), ('beer', 'NN')], [('None', 'NN')], [('open', 'JJ'), ('time', 'NN'), ('brings', 'VBZ'), ('couch', 'JJ')]], [[('Told', 'NNP'), ('mom', 'NN'), ('hit', 'VBD'), ('Twitter', 'NNP'), ('follower', 'NN')], [('She', 'PRP'), ('pointed', 'VBD'), ('brother', 'RB'), ('owns', 'VBZ'), ('house', 'NN'), ('wanted', 'VBD'), ('several', 'JJ'), ('collection', 'NN'), ('agency', 'NN')], []], [[('Roses', 'NNS'), ('dead', 'VBP')], [('Love', 'NNP'), ('fake', 'NN')], [('Weddings', 'NNS'), ('basically', 'RB'), ('funeral', 'JJ'), ('cake', 'NN')]]]\n",
            "[0.625, 0.75]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VGRgqdRtZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cebe42f-43b8-4c31-d431-1e4ff4e6e774"
      },
      "source": [
        "text_data, senti_data, labels = [], [], []\n",
        "\n",
        "for i in range(0, len(df_result['is_humor'])):\n",
        "    text_data.append(formatted_texts[i])\n",
        "    senti_data.append(senti_score[i])\n",
        "    labels.append(df_result['offense_rating'][i])\n",
        "\n",
        "\n",
        "total = len(text_data)\n",
        "train_per = int(0.75 * total)\n",
        "\n",
        "train_data, train_labels = [], []\n",
        "test_data, test_labels = [], []\n",
        "\n",
        "for i in range(0, total):\n",
        "  if (i < train_per):\n",
        "    list = np.append(text_data[i], senti_data[i])\n",
        "    train_data.append(list)\n",
        "    train_labels.append(labels[i])\n",
        "  else:\n",
        "    list = np.append(text_data[i], senti_data[i])\n",
        "    test_data.append(list)\n",
        "    test_labels.append(labels[i])\n",
        "\n",
        "print(test_data[0])\n",
        "train_data = np.asarray(train_data)\n",
        "train_labels = np.asarray(train_labels)\n",
        "test_data = np.asarray(test_data)\n",
        "test_labels = np.asarray(test_labels)\n",
        "print(test_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.   933.  1006.   129.\n",
            "   500.    97.   342.    26.  5252. 10576.    67.   712.   174.   865.\n",
            "    53.  4276. 12998.     0.]\n",
            "[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.   933.  1006.   129.\n",
            "   500.    97.   342.    26.  5252. 10576.    67.   712.   174.   865.\n",
            "    53.  4276. 12998.     0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnUpTSxgnNnn",
        "outputId": "f443e0ff-f838-4e6e-d3a4-6a5c88fe3951"
      },
      "source": [
        "print(len(train_data[10]))\n",
        "print(len(test_data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8qi8eHivGOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bb8a55-f937-49f6-ce82-348f98a77baf"
      },
      "source": [
        "example = {'text': 'example'}\n",
        "print(example.get('text'))\n",
        "print(train_labels[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example\n",
            "[0.2 1.1 2.4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t39mozsxUyr5"
      },
      "source": [
        "#print(len(train_data))\n",
        "#print(len(test_data))\n",
        "np.save('/tmp/train_data', train_data)\n",
        "np.save('/tmp/train_labels', train_labels)\n",
        "np.save('/tmp/test_data', test_data)\n",
        "np.save('/tmp/test_labels', test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmQFMPvEbSmK"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "    tf.keras.layers.Dense(16, activation='relu', kernel_initializer='normal'),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(1, kernel_initializer='normal')\n",
        "])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TF-iXoKZito"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.mean_squared_error,\n",
        "              #optimizer=tf.keras.optimizers.SGD(learning_rate=0.000001, momentum=0.9),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.0005),\n",
        "              metrics=['mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKaecWqKZp3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a704b39-1f16-4fee-b324-c2b62a4e5d40"
      },
      "source": [
        "#print(train_data[0])\n",
        "x_data = train_data\n",
        "x_data.astype(float)\n",
        "y_data = train_labels\n",
        "y_data.astype(float)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  history = model.fit(x_data, y_data, epochs=1000, validation_split=0.3, verbose=1, callbacks=callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "132/132 [==============================] - 1s 3ms/step - loss: 316.8192 - mse: 316.8132 - val_loss: 93.3078 - val_mse: 93.3019\n",
            "Epoch 2/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 32.9408 - mse: 32.9350 - val_loss: 26.1497 - val_mse: 26.1439\n",
            "Epoch 3/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 10.4450 - mse: 10.4391 - val_loss: 11.5984 - val_mse: 11.5926\n",
            "Epoch 4/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 4.4700 - mse: 4.4642 - val_loss: 6.1307 - val_mse: 6.1249\n",
            "Epoch 5/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 2.7503 - mse: 2.7445 - val_loss: 4.2025 - val_mse: 4.1967\n",
            "Epoch 6/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 2.0489 - mse: 2.0431 - val_loss: 3.1513 - val_mse: 3.1455\n",
            "Epoch 7/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.6805 - mse: 1.6747 - val_loss: 2.5217 - val_mse: 2.5159\n",
            "Epoch 8/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.5330 - mse: 1.5272 - val_loss: 2.0848 - val_mse: 2.0790\n",
            "Epoch 9/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.3561 - mse: 1.3504 - val_loss: 1.9258 - val_mse: 1.9201\n",
            "Epoch 10/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.2552 - mse: 1.2495 - val_loss: 1.7704 - val_mse: 1.7647\n",
            "Epoch 11/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.2050 - mse: 1.1992 - val_loss: 1.6516 - val_mse: 1.6458\n",
            "Epoch 12/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.1628 - mse: 1.1571 - val_loss: 1.5364 - val_mse: 1.5307\n",
            "Epoch 13/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.1257 - mse: 1.1199 - val_loss: 1.5236 - val_mse: 1.5179\n",
            "Epoch 14/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.1017 - mse: 1.0959 - val_loss: 1.4123 - val_mse: 1.4065\n",
            "Epoch 15/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0761 - mse: 1.0704 - val_loss: 1.4122 - val_mse: 1.4064\n",
            "Epoch 16/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0591 - mse: 1.0533 - val_loss: 1.3830 - val_mse: 1.3772\n",
            "Epoch 17/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0489 - mse: 1.0432 - val_loss: 1.2950 - val_mse: 1.2893\n",
            "Epoch 18/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0272 - mse: 1.0215 - val_loss: 1.2741 - val_mse: 1.2684\n",
            "Epoch 19/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0298 - mse: 1.0241 - val_loss: 1.2319 - val_mse: 1.2262\n",
            "Epoch 20/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0122 - mse: 1.0064 - val_loss: 1.2540 - val_mse: 1.2483\n",
            "Epoch 21/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0216 - mse: 1.0159 - val_loss: 1.2114 - val_mse: 1.2057\n",
            "Epoch 22/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 1.0024 - mse: 0.9967 - val_loss: 1.1751 - val_mse: 1.1694\n",
            "Epoch 23/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9925 - mse: 0.9867 - val_loss: 1.1753 - val_mse: 1.1696\n",
            "Epoch 24/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9983 - mse: 0.9926 - val_loss: 1.2057 - val_mse: 1.2000\n",
            "Epoch 25/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9922 - mse: 0.9865 - val_loss: 1.1925 - val_mse: 1.1868\n",
            "Epoch 26/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9820 - mse: 0.9763 - val_loss: 1.1504 - val_mse: 1.1447\n",
            "Epoch 27/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9731 - mse: 0.9674 - val_loss: 1.1667 - val_mse: 1.1610\n",
            "Epoch 28/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9714 - mse: 0.9657 - val_loss: 1.1185 - val_mse: 1.1128\n",
            "Epoch 29/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9700 - mse: 0.9643 - val_loss: 1.1104 - val_mse: 1.1047\n",
            "Epoch 30/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9639 - mse: 0.9583 - val_loss: 1.1156 - val_mse: 1.1099\n",
            "Epoch 31/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9722 - mse: 0.9666 - val_loss: 1.1298 - val_mse: 1.1241\n",
            "Epoch 32/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9638 - mse: 0.9581 - val_loss: 1.1018 - val_mse: 1.0961\n",
            "Epoch 33/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9700 - mse: 0.9643 - val_loss: 1.1167 - val_mse: 1.1110\n",
            "Epoch 34/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9640 - mse: 0.9583 - val_loss: 1.0998 - val_mse: 1.0942\n",
            "Epoch 35/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9541 - mse: 0.9485 - val_loss: 1.1062 - val_mse: 1.1005\n",
            "Epoch 36/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9625 - mse: 0.9569 - val_loss: 1.1068 - val_mse: 1.1011\n",
            "Epoch 37/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9538 - mse: 0.9482 - val_loss: 1.0931 - val_mse: 1.0874\n",
            "Epoch 38/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9507 - mse: 0.9451 - val_loss: 1.0725 - val_mse: 1.0668\n",
            "Epoch 39/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9572 - mse: 0.9516 - val_loss: 1.1614 - val_mse: 1.1558\n",
            "Epoch 40/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9461 - mse: 0.9405 - val_loss: 1.1403 - val_mse: 1.1347\n",
            "Epoch 41/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9486 - mse: 0.9429 - val_loss: 1.0714 - val_mse: 1.0657\n",
            "Epoch 42/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9439 - mse: 0.9383 - val_loss: 1.0914 - val_mse: 1.0858\n",
            "Epoch 43/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9492 - mse: 0.9436 - val_loss: 1.0815 - val_mse: 1.0759\n",
            "Epoch 44/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9454 - mse: 0.9398 - val_loss: 1.0733 - val_mse: 1.0677\n",
            "Epoch 45/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9451 - mse: 0.9395 - val_loss: 1.0871 - val_mse: 1.0815\n",
            "Epoch 46/1000\n",
            "132/132 [==============================] - 0s 2ms/step - loss: 0.9405 - mse: 0.9349 - val_loss: 1.0960 - val_mse: 1.0904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFOeJIs57BI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0defddcb-9433-4b11-d67e-7f4963892cf6"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  test_results = model.evaluate(test_data, test_labels, verbose=1)\n",
        "print('Test results: {}'.format(test_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 999us/step - loss: 1.1558 - mse: 1.1503\n",
            "Test results: [1.1558436155319214, 1.1502561569213867]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEjhH9FmXo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92efec27-31fa-4fe3-cffc-01ae1615f0cb"
      },
      "source": [
        "test_predictions = model.predict(test_data)\n",
        "print(test_predictions[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6085428 ]\n",
            " [0.5951169 ]\n",
            " [0.5357747 ]\n",
            " [0.5680758 ]\n",
            " [0.7210351 ]\n",
            " [0.5951169 ]\n",
            " [0.52977264]\n",
            " [0.6040856 ]\n",
            " [0.57885075]\n",
            " [0.58686763]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "uye6wKNuoRcI",
        "outputId": "a6f02552-d89d-42bf-8492-b13d61f10a7b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 9]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}