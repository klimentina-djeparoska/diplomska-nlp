{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "humor_rating_simple_model_dense_layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmhHcKcjPapl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ffd9dd6-5cf2-4af6-a15a-74513027ec9e"
      },
      "source": [
        "# connect with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFiBr2XqSGD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855d603c-df8b-4ed9-bec1-9609bee41448"
      },
      "source": [
        "!pip install keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "import string\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK3CQCMCQKF_"
      },
      "source": [
        "def load_dataset():\n",
        "  column_names = [ 'id', 'text', 'is_humor', 'humor_rating', 'humor_controversy', 'offense_rating' ]\n",
        "  data_path = '/content/drive/My Drive/data-nlp-humor-offense/train.csv'\n",
        "  data_frame = pd.read_csv(data_path, names=column_names, skiprows=1, na_values=\"?\", sep=\",\", skipinitialspace=True)\n",
        "  data = data_frame.fillna(0).to_numpy()\n",
        "\n",
        "  return data, data_frame"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdL8dNRjQto-",
        "outputId": "622edca4-49a3-4b03-ca8e-8cfe572dcc19"
      },
      "source": [
        "result, df_result = load_dataset()\n",
        "print(result[:5])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1\n",
            "  \"TENNESSEE: We're the best state. Nobody even comes close. *Elevennessee walks into the room* TENNESSEE: Oh shit...\"\n",
            "  1 2.42 1.0 0.2]\n",
            " [2\n",
            "  'A man inserted an advertisement in the classifieds \"Wife Wanted\". The next day, he received 1000 of replies, all reading: \"You can have mine.\" Free delivery also available at your door step'\n",
            "  1 2.5 1.0 1.1]\n",
            " [3\n",
            "  'How many men does it take to open a can of beer? None. It should be open by the time she brings it to the couch.'\n",
            "  1 1.95 0.0 2.4]\n",
            " [4\n",
            "  \"Told my mom I hit 1200 Twitter followers. She pointed out how my brother owns a house and I'm wanted by several collection agencies. Oh ma!\"\n",
            "  1 2.11 1.0 0.0]\n",
            " [5\n",
            "  'Roses are dead. Love is fake. Weddings are basically funerals with cake.'\n",
            "  1 2.78 0.0 0.1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXTpvXFcfo1x",
        "outputId": "eb4a2b61-396e-407c-9523-e9a74c50423d"
      },
      "source": [
        "print(df_result['is_humor'].value_counts())\n",
        "print(df_result['humor_rating'].value_counts())\n",
        "print(df_result['humor_controversy'].value_counts())\n",
        "print(df_result['offense_rating'].value_counts())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    4932\n",
            "0    3068\n",
            "Name: is_humor, dtype: int64\n",
            "2.00    156\n",
            "2.50    125\n",
            "2.05    111\n",
            "2.60    109\n",
            "2.25    101\n",
            "       ... \n",
            "3.13      1\n",
            "0.47      1\n",
            "2.57      1\n",
            "3.74      1\n",
            "0.30      1\n",
            "Name: humor_rating, Length: 253, dtype: int64\n",
            "0.0    2467\n",
            "1.0    2465\n",
            "Name: humor_controversy, dtype: int64\n",
            "0.00    3388\n",
            "0.15     394\n",
            "0.05     387\n",
            "0.10     333\n",
            "0.20     306\n",
            "        ... \n",
            "4.85       2\n",
            "4.80       2\n",
            "4.55       2\n",
            "4.75       1\n",
            "4.45       1\n",
            "Name: offense_rating, Length: 98, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Go-yTSizFd"
      },
      "source": [
        "def format_text_to_one_hot(array_of_texts):\n",
        "  vocabulary = []\n",
        "  vocabulary_map = {}\n",
        "  counter = 1\n",
        "  end_result = []\n",
        "  texts_scores = []\n",
        "\n",
        "  for text in array_of_texts:\n",
        "    \n",
        "    text_represented_with_numbers = []\n",
        "    positive_text_score = 0\n",
        "    negative_text_score = 0\n",
        "    # break the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # break the sentence into words\n",
        "      words_array = word_tokenize(sentence)\n",
        "\n",
        "      for word in words_array:\n",
        "\n",
        "        # remove punctuation\n",
        "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "        \n",
        "        # lower case all letters \n",
        "        word = word.lower()\n",
        "\n",
        "        if word != '' and word != 's':\n",
        "          if not vocabulary.__contains__(word):\n",
        "            vocabulary.append(word)\n",
        "            vocabulary_map[word] = counter\n",
        "            counter += 1\n",
        "          \n",
        "          text_represented_with_numbers.append(vocabulary_map[word])\n",
        "\n",
        "        # add positive negative score\n",
        "\n",
        "    # add text to result array\n",
        "    end_result.append(text_represented_with_numbers)\n",
        "   # difference = positive_text_score/negative_text_score\n",
        "   # texts_scores.append(difference)\n",
        "  \n",
        "  # arrays to have same length\n",
        "  padded_seq = pad_sequences(end_result, maxlen=63, dtype='int32', padding='pre', value=0.0)\n",
        "  print(len(vocabulary))\n",
        "  return padded_seq"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gy0eR2WK4gX"
      },
      "source": [
        "# n - NOUN\n",
        "#v - VERB\n",
        "#a - ADJECTIVE\n",
        "#s - ADJECTIVE SATELLITE\n",
        "#r - ADVERB\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
        "def get_sentiment(word,tag):\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    \n",
        "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        return []\n",
        "\n",
        "    #Lemmatization\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    if not lemma:\n",
        "        return []\n",
        "\n",
        "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
        "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
        "    #Some of the words have only one Synset and some have several.\n",
        "    synsets = wn.synsets(word, pos=wn_tag)\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
        "\n",
        "    pos=neg=obj=count=0\n",
        "    \n",
        "\n",
        "def getPostTags(array_of_texts):\n",
        "  vocabulary = []\n",
        "  vocabulary_map = {}\n",
        "  counter = 1\n",
        "  end_result = []\n",
        "  texts_scores = []\n",
        "  postagging = []\n",
        "\n",
        "  for text in array_of_texts:\n",
        "    \n",
        "    text_represented_with_numbers = []\n",
        "    positive_text_score = 0\n",
        "    negative_text_score = 0\n",
        "    # break the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    posttagging_sentences = []\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # break the sentence into words\n",
        "      words_array = word_tokenize(sentence)\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      filtered_sentence = [w for w in words_array if not w in stop_words] \n",
        "\n",
        "      # remove punctuation\n",
        "      word_tokens2 = [w for w in filtered_sentence if not w in string.punctuation]\n",
        "      # lower case all letters \n",
        "      lower_case_sentance = map(lambda w: w.lower(), word_tokens2)\n",
        "      #lemmatization\n",
        "      lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
        "        \n",
        "      # Remove characters which have length less than 2  \n",
        "      without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
        "      # Remove numbers\n",
        "      cleaned_data = [word for word in without_single_chr if not word.isnumeric()]\n",
        "      \n",
        "      posttagging_sentences.append(nltk.pos_tag(cleaned_data))\n",
        "    \n",
        "    postagging.append(posttagging_sentences)\n",
        "  return postagging\n",
        "\n",
        "    ###################################################################################\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2aHzQdWn9aM"
      },
      "source": [
        "# value = [\"Something, this is an example. One!\", \"This is two!! Really, that's exactly what i though.\"]\n",
        "# format_text_to_one_hot(value)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U-OqRxCqIe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e027e15-956a-44bf-83e9-eb5b4924c3e6"
      },
      "source": [
        "texts = df_result['text']\n",
        "values = df_result['humor_rating']\n",
        "np.save('/tmp/values', values)\n",
        "print(len(texts))\n",
        "with tf.device('/GPU:0'):\n",
        "  formatted_texts = format_text_to_one_hot(texts)\n",
        "np.save('/tmp/formatted_texts', formatted_texts)\n",
        "print(len(formatted_texts))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "15050\n",
            "8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObxWn1kpgigB",
        "outputId": "e1728400-bedb-4df4-d7c8-a37ec271db5b"
      },
      "source": [
        "senti_score = []\n",
        "pos=neg=obj=count=0\n",
        "with tf.device('/GPU:0'):\n",
        "  posttagging = getPostTags(texts)\n",
        "print(posttagging[:5])\n",
        "count1 = 1\n",
        "for text in posttagging:\n",
        "  senti_scores_text = 0\n",
        "  for sentence in text:\n",
        "    \n",
        "    senti_val = [get_sentiment(x,y) for (x,y) in sentence]\n",
        "    for score in senti_val:\n",
        "      try:\n",
        "        pos = pos + score[1]  #positive score is stored at 2nd position\n",
        "        neg = neg + score[2]  #negative score is stored at 3rd position\n",
        "      except:\n",
        "        continue\n",
        "    result_score = pos - neg\n",
        "    senti_scores_text += result_score \n",
        "    pos=neg=0 \n",
        "  senti_score.append(senti_scores_text)\n",
        "\n",
        "print(senti_score[:2])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[('TENNESSEE', 'NNP'), (\"'re\", 'VBP'), ('best', 'JJS'), ('state', 'NN')], [('Nobody', 'NN'), ('even', 'RB'), ('come', 'VBP'), ('close', 'RB')], [('*Elevennessee', 'NN'), ('walk', 'NN'), ('room*', 'NN'), ('TENNESSEE', 'NNP'), ('shit', 'NN'), ('...', ':')]], [[('man', 'NN'), ('inserted', 'VBD'), ('advertisement', 'JJ'), ('classified', 'JJ'), ('Wife', 'NNP'), ('Wanted', 'VBD')], [('The', 'DT'), ('next', 'JJ'), ('day', 'NN'), ('received', 'VBD'), ('reply', 'RB'), ('reading', 'VBG'), ('You', 'PRP'), ('mine', 'VBP')], [('Free', 'JJ'), ('delivery', 'NN'), ('also', 'RB'), ('available', 'JJ'), ('door', 'JJ'), ('step', 'NN')]], [[('How', 'WRB'), ('many', 'JJ'), ('men', 'NNS'), ('take', 'VBP'), ('open', 'JJ'), ('beer', 'NN')], [('None', 'NN')], [('open', 'JJ'), ('time', 'NN'), ('brings', 'VBZ'), ('couch', 'JJ')]], [[('Told', 'NNP'), ('mom', 'NN'), ('hit', 'VBD'), ('Twitter', 'NNP'), ('follower', 'NN')], [('She', 'PRP'), ('pointed', 'VBD'), ('brother', 'RB'), ('owns', 'VBZ'), ('house', 'NN'), ('wanted', 'VBD'), ('several', 'JJ'), ('collection', 'NN'), ('agency', 'NN')], []], [[('Roses', 'NNS'), ('dead', 'VBP')], [('Love', 'NNP'), ('fake', 'NN')], [('Weddings', 'NNS'), ('basically', 'RB'), ('funeral', 'JJ'), ('cake', 'NN')]]]\n",
            "[0.625, 0.75]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VGRgqdRtZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d04afee-d825-4fc6-d747-f24873e79f34"
      },
      "source": [
        "text_data, senti_data, labels = [], [], []\n",
        "\n",
        "for i in range(0, len(df_result['is_humor'])):\n",
        "  if df_result['is_humor'][i] == 1:\n",
        "    text_data.append(formatted_texts[i])\n",
        "    senti_data.append(senti_score[i])\n",
        "    labels.append(df_result['humor_rating'][i])\n",
        "\n",
        "\n",
        "total = len(text_data)\n",
        "train_per = int(0.75 * total)\n",
        "\n",
        "train_data, train_labels = [], []\n",
        "test_data, test_labels = [], []\n",
        "\n",
        "for i in range(0, total):\n",
        "  if (i < train_per):\n",
        "    list = np.append(text_data[i], senti_data[i])\n",
        "    train_data.append(list)\n",
        "    train_labels.append(labels[i])\n",
        "  else:\n",
        "    list = np.append(text_data[i], senti_data[i])\n",
        "    test_data.append(list)\n",
        "    test_labels.append(labels[i])\n",
        "\n",
        "print(test_data[0])\n",
        "train_data = np.asarray(train_data)\n",
        "train_labels = np.asarray(train_labels)\n",
        "test_data = np.asarray(test_data)\n",
        "test_labels = np.asarray(test_labels)\n",
        "print(test_data[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  6.5000e+01  2.3890e+03  5.2000e+01  8.7000e+01  3.5000e+01\n",
            "  1.0060e+03  3.0000e+00  1.7000e+01  5.5200e+02  5.9050e+03  3.4300e+03\n",
            "  2.2300e+02  4.0000e+00  7.6370e+03  3.5000e+01  3.5100e+02  3.2920e+03\n",
            "  7.1110e+03  1.2992e+04  2.9000e+02  2.0000e+00  2.9300e+02  1.1300e+02\n",
            "  1.6000e+01  1.2993e+04  7.3000e+01 -5.0000e-01]\n",
            "[ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
            "  0.0000e+00  6.5000e+01  2.3890e+03  5.2000e+01  8.7000e+01  3.5000e+01\n",
            "  1.0060e+03  3.0000e+00  1.7000e+01  5.5200e+02  5.9050e+03  3.4300e+03\n",
            "  2.2300e+02  4.0000e+00  7.6370e+03  3.5000e+01  3.5100e+02  3.2920e+03\n",
            "  7.1110e+03  1.2992e+04  2.9000e+02  2.0000e+00  2.9300e+02  1.1300e+02\n",
            "  1.6000e+01  1.2993e+04  7.3000e+01 -5.0000e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnUpTSxgnNnn",
        "outputId": "d789866e-05c3-4cf1-c01f-8b88911d2a99"
      },
      "source": [
        "print(len(train_data[10]))\n",
        "print(len(test_data[0]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8qi8eHivGOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d580750f-6a89-4d74-f148-172396b56d5b"
      },
      "source": [
        "example = {'text': 'example'}\n",
        "print(example.get('text'))\n",
        "print(train_labels[:3])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example\n",
            "[2.42 2.5  1.95]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t39mozsxUyr5"
      },
      "source": [
        "#print(len(train_data))\n",
        "#print(len(test_data))\n",
        "np.save('/tmp/train_data', train_data)\n",
        "np.save('/tmp/train_labels', train_labels)\n",
        "np.save('/tmp/test_data', test_data)\n",
        "np.save('/tmp/test_labels', test_labels)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmQFMPvEbSmK"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "    tf.keras.layers.Dense(16, activation='relu', kernel_initializer='normal'),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(1, kernel_initializer='normal')\n",
        "])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='auto')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TF-iXoKZito"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.mean_squared_error,\n",
        "              #optimizer=tf.keras.optimizers.SGD(learning_rate=0.000001, momentum=0.9),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.0005),\n",
        "              metrics=['mse'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKaecWqKZp3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5a673e-2446-455b-ef53-5e1a49ffa4bf"
      },
      "source": [
        "#print(train_data[0])\n",
        "x_data = train_data\n",
        "x_data.astype(float)\n",
        "y_data = train_labels\n",
        "y_data.astype(float)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  history = model.fit(x_data, y_data, epochs=1000, validation_split=0.3, verbose=1, callbacks=callback)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "81/81 [==============================] - 1s 3ms/step - loss: 3070.4043 - mse: 3070.3982 - val_loss: 775.4647 - val_mse: 775.4586\n",
            "Epoch 2/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 328.8065 - mse: 328.8004 - val_loss: 378.0568 - val_mse: 378.0507\n",
            "Epoch 3/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 160.2143 - mse: 160.2083 - val_loss: 215.1990 - val_mse: 215.1930\n",
            "Epoch 4/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 86.2476 - mse: 86.2416 - val_loss: 130.8337 - val_mse: 130.8277\n",
            "Epoch 5/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 51.5747 - mse: 51.5687 - val_loss: 89.0601 - val_mse: 89.0542\n",
            "Epoch 6/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 33.0793 - mse: 33.0733 - val_loss: 66.5016 - val_mse: 66.4957\n",
            "Epoch 7/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 22.3257 - mse: 22.3198 - val_loss: 51.3649 - val_mse: 51.3589\n",
            "Epoch 8/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 15.7111 - mse: 15.7052 - val_loss: 40.8819 - val_mse: 40.8760\n",
            "Epoch 9/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 11.8710 - mse: 11.8651 - val_loss: 33.6036 - val_mse: 33.5977\n",
            "Epoch 10/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 9.3116 - mse: 9.3057 - val_loss: 28.6628 - val_mse: 28.6569\n",
            "Epoch 11/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 7.1493 - mse: 7.1434 - val_loss: 24.1631 - val_mse: 24.1572\n",
            "Epoch 12/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 5.6962 - mse: 5.6903 - val_loss: 21.9467 - val_mse: 21.9408\n",
            "Epoch 13/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 5.0176 - mse: 5.0117 - val_loss: 18.3210 - val_mse: 18.3151\n",
            "Epoch 14/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 4.2843 - mse: 4.2784 - val_loss: 17.1169 - val_mse: 17.1110\n",
            "Epoch 15/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 3.6760 - mse: 3.6701 - val_loss: 15.5102 - val_mse: 15.5043\n",
            "Epoch 16/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 3.1906 - mse: 3.1847 - val_loss: 14.1501 - val_mse: 14.1442\n",
            "Epoch 17/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.9859 - mse: 2.9800 - val_loss: 12.6890 - val_mse: 12.6832\n",
            "Epoch 18/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.7054 - mse: 2.6995 - val_loss: 11.9234 - val_mse: 11.9175\n",
            "Epoch 19/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.5595 - mse: 2.5537 - val_loss: 11.2493 - val_mse: 11.2435\n",
            "Epoch 20/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.3963 - mse: 2.3904 - val_loss: 11.1264 - val_mse: 11.1205\n",
            "Epoch 21/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.2635 - mse: 2.2576 - val_loss: 10.3865 - val_mse: 10.3806\n",
            "Epoch 22/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.1896 - mse: 2.1837 - val_loss: 9.7228 - val_mse: 9.7169\n",
            "Epoch 23/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.0086 - mse: 2.0027 - val_loss: 9.6577 - val_mse: 9.6518\n",
            "Epoch 24/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.0100 - mse: 2.0041 - val_loss: 9.1176 - val_mse: 9.1117\n",
            "Epoch 25/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.8681 - mse: 1.8622 - val_loss: 9.3790 - val_mse: 9.3731\n",
            "Epoch 26/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.8456 - mse: 1.8397 - val_loss: 8.9384 - val_mse: 8.9325\n",
            "Epoch 27/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.8186 - mse: 1.8127 - val_loss: 8.6452 - val_mse: 8.6394\n",
            "Epoch 28/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.7519 - mse: 1.7460 - val_loss: 8.0340 - val_mse: 8.0281\n",
            "Epoch 29/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.7280 - mse: 1.7221 - val_loss: 8.4349 - val_mse: 8.4290\n",
            "Epoch 30/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.6648 - mse: 1.6590 - val_loss: 8.3765 - val_mse: 8.3706\n",
            "Epoch 31/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.5867 - mse: 1.5808 - val_loss: 7.5167 - val_mse: 7.5109\n",
            "Epoch 32/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.5890 - mse: 1.5831 - val_loss: 8.2599 - val_mse: 8.2540\n",
            "Epoch 33/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.6221 - mse: 1.6163 - val_loss: 7.6696 - val_mse: 7.6637\n",
            "Epoch 34/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.6270 - mse: 1.6211 - val_loss: 7.3088 - val_mse: 7.3029\n",
            "Epoch 35/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.5965 - mse: 1.5906 - val_loss: 7.1632 - val_mse: 7.1573\n",
            "Epoch 36/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.5034 - mse: 1.4976 - val_loss: 6.8070 - val_mse: 6.8012\n",
            "Epoch 37/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.4628 - mse: 1.4570 - val_loss: 7.0377 - val_mse: 7.0318\n",
            "Epoch 38/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.4301 - mse: 1.4243 - val_loss: 7.1318 - val_mse: 7.1259\n",
            "Epoch 39/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.4642 - mse: 1.4583 - val_loss: 6.5755 - val_mse: 6.5697\n",
            "Epoch 40/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.4444 - mse: 1.4385 - val_loss: 6.9561 - val_mse: 6.9503\n",
            "Epoch 41/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.3953 - mse: 1.3895 - val_loss: 6.8416 - val_mse: 6.8357\n",
            "Epoch 42/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.3494 - mse: 1.3436 - val_loss: 7.1205 - val_mse: 7.1147\n",
            "Epoch 43/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.3442 - mse: 1.3384 - val_loss: 6.3441 - val_mse: 6.3382\n",
            "Epoch 44/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.3582 - mse: 1.3523 - val_loss: 7.3197 - val_mse: 7.3138\n",
            "Epoch 45/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.3290 - mse: 1.3231 - val_loss: 6.4873 - val_mse: 6.4815\n",
            "Epoch 46/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.3050 - mse: 1.2992 - val_loss: 6.5853 - val_mse: 6.5794\n",
            "Epoch 47/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2454 - mse: 1.2395 - val_loss: 6.1741 - val_mse: 6.1683\n",
            "Epoch 48/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2588 - mse: 1.2529 - val_loss: 6.9563 - val_mse: 6.9504\n",
            "Epoch 49/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2524 - mse: 1.2466 - val_loss: 6.8011 - val_mse: 6.7952\n",
            "Epoch 50/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2794 - mse: 1.2735 - val_loss: 6.1805 - val_mse: 6.1747\n",
            "Epoch 51/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2251 - mse: 1.2192 - val_loss: 6.1532 - val_mse: 6.1473\n",
            "Epoch 52/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2089 - mse: 1.2030 - val_loss: 6.4771 - val_mse: 6.4713\n",
            "Epoch 53/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.2160 - mse: 1.2101 - val_loss: 6.6280 - val_mse: 6.6221\n",
            "Epoch 54/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.1536 - mse: 1.1478 - val_loss: 6.0912 - val_mse: 6.0854\n",
            "Epoch 55/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.1899 - mse: 1.1840 - val_loss: 6.2170 - val_mse: 6.2111\n",
            "Epoch 56/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.1454 - mse: 1.1395 - val_loss: 6.2539 - val_mse: 6.2481\n",
            "Epoch 57/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.1182 - mse: 1.1124 - val_loss: 5.9747 - val_mse: 5.9688\n",
            "Epoch 58/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.1356 - mse: 1.1297 - val_loss: 6.0918 - val_mse: 6.0859\n",
            "Epoch 59/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.1181 - mse: 1.1123 - val_loss: 6.0469 - val_mse: 6.0410\n",
            "Epoch 60/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.0857 - mse: 1.0799 - val_loss: 5.8655 - val_mse: 5.8597\n",
            "Epoch 61/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.0948 - mse: 1.0890 - val_loss: 5.6280 - val_mse: 5.6222\n",
            "Epoch 62/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.0592 - mse: 1.0534 - val_loss: 5.8232 - val_mse: 5.8174\n",
            "Epoch 63/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.0343 - mse: 1.0284 - val_loss: 5.7117 - val_mse: 5.7059\n",
            "Epoch 64/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.0468 - mse: 1.0410 - val_loss: 5.8305 - val_mse: 5.8247\n",
            "Epoch 65/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9815 - mse: 0.9756 - val_loss: 5.7029 - val_mse: 5.6971\n",
            "Epoch 66/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 1.0053 - mse: 0.9995 - val_loss: 5.5933 - val_mse: 5.5875\n",
            "Epoch 67/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9663 - mse: 0.9605 - val_loss: 5.8062 - val_mse: 5.8004\n",
            "Epoch 68/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9406 - mse: 0.9348 - val_loss: 5.6013 - val_mse: 5.5955\n",
            "Epoch 69/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9260 - mse: 0.9202 - val_loss: 5.3084 - val_mse: 5.3026\n",
            "Epoch 70/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9056 - mse: 0.8998 - val_loss: 5.3386 - val_mse: 5.3328\n",
            "Epoch 71/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.9006 - mse: 0.8948 - val_loss: 5.4798 - val_mse: 5.4740\n",
            "Epoch 72/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8852 - mse: 0.8794 - val_loss: 5.4563 - val_mse: 5.4505\n",
            "Epoch 73/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8864 - mse: 0.8806 - val_loss: 5.6911 - val_mse: 5.6853\n",
            "Epoch 74/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8370 - mse: 0.8312 - val_loss: 5.2856 - val_mse: 5.2798\n",
            "Epoch 75/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8561 - mse: 0.8503 - val_loss: 5.2533 - val_mse: 5.2475\n",
            "Epoch 76/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8234 - mse: 0.8176 - val_loss: 5.5197 - val_mse: 5.5139\n",
            "Epoch 77/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8049 - mse: 0.7991 - val_loss: 5.4552 - val_mse: 5.4494\n",
            "Epoch 78/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.8184 - mse: 0.8126 - val_loss: 5.5332 - val_mse: 5.5274\n",
            "Epoch 79/1000\n",
            "81/81 [==============================] - 0s 1ms/step - loss: 0.7841 - mse: 0.7783 - val_loss: 5.2724 - val_mse: 5.2666\n",
            "Epoch 80/1000\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.7509 - mse: 0.7451 - val_loss: 5.3215 - val_mse: 5.3157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFOeJIs57BI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c668812a-9a29-41b4-84cd-d6f971f77855"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  test_results = model.evaluate(test_data, test_labels, verbose=1)\n",
        "print('Test results: {}'.format(test_results))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39/39 [==============================] - 0s 930us/step - loss: 4.8918 - mse: 4.8860\n",
            "Test results: [4.891822814941406, 4.886046886444092]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEjhH9FmXo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3a7886-5089-478d-9fd2-ce135606dcaf"
      },
      "source": [
        "test_predictions = model.predict(test_data)\n",
        "print(test_predictions[:10])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.9366903]\n",
            " [4.0895624]\n",
            " [2.4577222]\n",
            " [1.8031048]\n",
            " [2.0170724]\n",
            " [4.962653 ]\n",
            " [2.115685 ]\n",
            " [3.8651028]\n",
            " [2.981954 ]\n",
            " [2.9555585]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "uye6wKNuoRcI",
        "outputId": "310095c6-7766-454f-bfca-b8bd03a3ea17"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 7]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}